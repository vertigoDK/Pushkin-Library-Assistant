import os
import shutil
from uuid import uuid4
from typing import List
from .get_embedings_function import get_embedding_function
from langchain_chroma import Chroma
from langchain_core.documents import Document


class VectorDatabaseHandlers:
    
    CHROMA_PATH = "./chroma_langchain_db"

    def __init__(self, collection_name: str):
        embedding = get_embedding_function()
        self._db = Chroma(
            collection_name=collection_name,
            embedding_function=embedding,
            persist_directory=self.CHROMA_PATH
        )


    def upsert_chroma(self, documents: List[Document]):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö, –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç."""
        try:
            # –ü–æ–ª—É—á–∞–µ–º —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã –∏–∑ –±–∞–∑—ã
            existing_items = self._db.get(include=["metadatas"])
            if existing_items is None:
                existing_items = {"metadatas": []}

            # –°–æ–±–∏—Ä–∞–µ–º —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            existing_ids = set(item["id"] for item in existing_items["metadatas"] if "id" in item)
            print(f"Number of existing documents in DB: {len(existing_ids)}")

            # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ ID –¥–ª—è –Ω–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            chunks_with_ids = self.calculate_chunk_ids(documents)

            # –¢–æ–ª—å–∫–æ –¥–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã—Ö –µ—â–µ –Ω–µ—Ç –≤ –±–∞–∑–µ
            new_chunks = [chunk for chunk in chunks_with_ids if chunk.metadata["id"] not in existing_ids]

            if new_chunks:
                print(f"üëâ Adding new documents: {len(new_chunks)}")
                # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ ID –¥–ª—è –Ω–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                new_chunk_ids = [chunk.metadata["id"] for chunk in new_chunks]
                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
                self._db.add_documents(new_chunks, ids=new_chunk_ids)
            else:
                print("‚úÖ No new documents to add")
                
        except Exception as e:
            print(f"‚ö†Ô∏è Error while upserting documents: {e}")

    def calculate_chunk_ids(self, chunks: List[Document]):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö ID –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞."""
        last_page_id = None
        current_chunk_index = 0

        for chunk in chunks:
            source = chunk.metadata.get("source")
            page = chunk.metadata.get("page")
            current_page_id = f"{source}:{page}"

            # –ï—Å–ª–∏ —ç—Ç–æ —Ç–æ—Ç –∂–µ –¥–æ–∫—É–º–µ–Ω—Ç, —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å —á–∞—Å—Ç–∏
            if current_page_id == last_page_id:
                current_chunk_index += 1
            else:
                current_chunk_index = 0

            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ ID –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            chunk_id = f"{current_page_id}:{current_chunk_index}"
            last_page_id = current_page_id

            # –î–æ–±–∞–≤–ª—è–µ–º ID –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            chunk.metadata["id"] = chunk_id

        return chunks

    def search_in_database(self, query_text: str):
        results = self._db.similarity_search(query_text, k=3)
        return results


if __name__ == '__main__':
    # –ü—Ä–∏–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞
    document_1 = Document(
        page_content="I had chocolate chip pancakes and scrambled eggs for breakfast this morning.",
        metadata={"source": "tweet", "page": 1},
        id=1,
    )

    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    vDatabaseHandler = VectorDatabaseHandlers("legends_collection")
    
    vDatabaseHandler.upsert_chroma([document_1])
